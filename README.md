# 🧀 Panic-Demo: Emergency Psychological Counseling Chatbot

This project implements a chatbot system for providing **Psychological First Aid (PFA)** to individuals experiencing panic attacks. It integrates a **vLLM-based generation server** with a **FastAPI web interface**, simulating real-time crisis counseling interactions.

---

## 📁 Project Structure

```
.
├── src/                            # Core application code
│   ├── chatbot.py                  # FastAPI app for dialogue & session management
│   ├── model.py                    # CounselorAgent definition
│   ├── logger.py, checker.py       # Utility modules
│   ├── model/                      # 🔻 Downloaded model directory (create manually)
│   └── ...
├── config/
│   ├── config.example.json         # 🔑 Template configuration file
│   └── config.json                 # ⚠️ Actual config (not tracked by Git)
├── static/, templates/             # Web interface files
├── run_chatbot.sh                  # Script to launch chatbot API server
├── run_vllm_server.sh              # Script to launch vLLM server
├── pyproject.toml                  # Python project configuration (uv-based)
├── uv.lock                         # Dependency lock file (generated by uv)
└── README.md
```

---

## ⚙️ Configuration Setup

1. Copy the example config file:

   ```bash
   cp config/config.example.json config/config.json
   ```

2. Edit `config.json` with your environment-specific values:

| Key                | Description                              |
| ------------------ | ---------------------------------------- |
| `chatbot_api_port` | Port for the FastAPI server (e.g., 8000) |
| `vllm_server_port` | Port for the vLLM server (e.g., 8001)    |
| `vllm_model_path`  | Path to local LLM model                  |
| `vllm_model_name`  | Internal name for model (e.g., `pacer`)  |
| `gemini_api_key`   | Google Gemini API key                    |
| `openai_api_key`   | OPENAI API key                           |
| `max_model_length` | Context window size                      |
| `max_new_tokens`   | Maximum tokens to generate per response  |

---

## 📦 Downloading the Model

Before running the vLLM server, you need to manually download the model into `src/model/`.

1. **Create the model directory:**

   ```bash
   mkdir -p src/model
   ```

2. **Request the model checkpoint** by sending an email to:  
   📧 [jihyunlee@postech.ac.kr](mailto:jihyunlee@postech.ac.kr)

3. **Unzip or place the model files** into the `src/model/` folder.

4. Make sure `config.json` has:

   ```json
   "vllm_model_path": "./src/model"
   ```

---

## 🚀 How to Run

### 1. Set up the Python environment (via [uv](https://github.com/astral-sh/uv))

```bash
uv pip install -e .
```

> This will install all dependencies based on `pyproject.toml`.

### 2. Start the vLLM model server

```bash
./run_vllm_server.sh
```

### 3. Start the FastAPI chatbot server

```bash
./run_chatbot.sh
```

Alternatively, you can run it manually:

```bash
CONFIG_PATH=config/config.json \
uvicorn src.chatbot:app --host 0.0.0.0 --port 8000 --reload
```


---

## 📚 API Documentation (Korean)

A detailed, always‑up‑to‑date specification—including request/response JSON schemas, example cURL commands, and error‑handling guidelines—is maintained in Notion:

[API Panic Counseling Chatbot Documentation](https://dolomite-beach-ce2.notion.site/API-Panic-Counseling-Chatbot-API-1c15483725bb809e9e79fbd1d0320f35)

### Key Endpoints

| Method | Path               | Purpose                                                                 |
| ------ | ------------------ | ----------------------------------------------------------------------- |
| `GET`  | `/status`          | Health‑check; returns `{ "ready": true/false }`                         |
| `POST` | `/init-session`    | Creates a new counseling session and returns the first system utterance |
| `POST` | `/chat`            | Sends a user utterance and receives the counselor reply                 |
| `GET`  | `/default-message` | Provides a sample user utterance for quick testing                      |

> ⚠️  The table above is a quick reference. **For payload examples, parameter details, and full error codes, please refer to the Notion link.**

---



## 🧪 Features

* ✅ Counselor utterance generation using local vLLM server
* ✅ Gemini-based safety filtering and naturalization
* ✅ Session-based dialogue tracking with TTL auto-expiry
* ✅ Conversation logging per session
* ✅ Web-based frontend interface (HTML/CSS/JS)

---

## 🔐 Security Notes

* Do **NOT** commit `config.json` — it may contain API keys or sensitive paths.
* The `.gitignore` file should exclude the config and logs:

  ```bash
  config/config.json
  logs/
  dials/
  ```

---

## 📄 License

MIT License
