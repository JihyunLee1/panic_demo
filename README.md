# ğŸ§€ Panic-Demo: Emergency Psychological Counseling Chatbot

This project implements a chatbot system for providing **Psychological First Aid (PFA)** to individuals experiencing panic attacks. It integrates a **vLLM-based generation server** with a **FastAPI web interface**, simulating real-time crisis counseling interactions.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ src/                            # Core application code
â”‚   â”œâ”€â”€ chatbot.py                  # FastAPI app for dialogue & session management
â”‚   â”œâ”€â”€ model.py                    # CounselorAgent definition
â”‚   â”œâ”€â”€ logger.py, checker.py       # Utility modules
â”‚   â”œâ”€â”€ model/                      # ğŸ”» Downloaded model directory (create manually)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.example.json         # ğŸ”‘ Template configuration file
â”‚   â””â”€â”€ config.json                 # âš ï¸ Actual config (not tracked by Git)
â”œâ”€â”€ static/, templates/             # Web interface files
â”œâ”€â”€ run_chatbot.sh                  # Script to launch chatbot API server
â”œâ”€â”€ run_vllm_server.sh              # Script to launch vLLM server
â”œâ”€â”€ pyproject.toml                  # Python project configuration (uv-based)
â”œâ”€â”€ uv.lock                         # Dependency lock file (generated by uv)
â””â”€â”€ README.md
```

---

## âš™ï¸ Configuration Setup

1. Copy the example config file:

   ```bash
   cp config/config.example.json config/config.json
   ```

2. Edit `config.json` with your environment-specific values:

| Key                | Description                              |
| ------------------ | ---------------------------------------- |
| `chatbot_api_port` | Port for the FastAPI server (e.g., 8000) |
| `vllm_server_port` | Port for the vLLM server (e.g., 8001)    |
| `vllm_model_path`  | Path to local LLM model                  |
| `vllm_model_name`  | Internal name for model (e.g., `pacer`)  |
| `gemini_api_key`   | Google Gemini API key                    |
| `openai_api_key`   | OPENAI API key                           |
| `max_model_length` | Context window size                      |
| `max_new_tokens`   | Maximum tokens to generate per response  |

---

## ğŸ“¦ Downloading the Model

Before running the vLLM server, you need to manually download the model into `src/model/`.

1. **Create the model directory:**

   ```bash
   mkdir -p src/model
   ```

2. **Request the model checkpoint** by sending an email to:  
   ğŸ“§ [jihyunlee@postech.ac.kr](mailto:jihyunlee@postech.ac.kr)

3. **Unzip or place the model files** into the `src/model/` folder.

4. Make sure `config.json` has:

   ```json
   "vllm_model_path": "./src/model"
   ```

---

## ğŸš€ How to Run

### 1. Set up the Python environment (via [uv](https://github.com/astral-sh/uv))

```bash
uv pip install -e .
```

> This will install all dependencies based on `pyproject.toml`.

### 2. Start the vLLM model server

```bash
./run_vllm_server.sh
```

### 3. Start the FastAPI chatbot server

```bash
./run_chatbot.sh
```

Alternatively, you can run it manually:

```bash
CONFIG_PATH=config/config.json \
uvicorn src.chatbot:app --host 0.0.0.0 --port 8000 --reload
```


---

## ğŸ“š API Documentation (Korean)

A detailed, alwaysâ€‘upâ€‘toâ€‘date specificationâ€”including request/response JSON schemas, example cURL commands, and errorâ€‘handling guidelinesâ€”is maintained in Notion:

[API Panic Counseling Chatbot Documentation](https://dolomite-beach-ce2.notion.site/API-Panic-Counseling-Chatbot-API-1c15483725bb809e9e79fbd1d0320f35)

### Key Endpoints

| Method | Path               | Purpose                                                                 |
| ------ | ------------------ | ----------------------------------------------------------------------- |
| `GET`  | `/status`          | Healthâ€‘check; returns `{ "ready": true/false }`                         |
| `POST` | `/init-session`    | Creates a new counseling session and returns the first system utterance |
| `POST` | `/chat`            | Sends a user utterance and receives the counselor reply                 |
| `GET`  | `/default-message` | Provides a sample user utterance for quick testing                      |

> âš ï¸  The table above is a quick reference. **For payload examples, parameter details, and full error codes, please refer to the Notion link.**

---



## ğŸ§ª Features

* âœ… Counselor utterance generation using local vLLM server
* âœ… Gemini-based safety filtering and naturalization
* âœ… Session-based dialogue tracking with TTL auto-expiry
* âœ… Conversation logging per session
* âœ… Web-based frontend interface (HTML/CSS/JS)

---

## ğŸ” Security Notes

* Do **NOT** commit `config.json` â€” it may contain API keys or sensitive paths.
* The `.gitignore` file should exclude the config and logs:

  ```bash
  config/config.json
  logs/
  dials/
  ```

---

## ğŸ“„ License

MIT License
